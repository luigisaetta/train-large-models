{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c92b860b",
   "metadata": {},
   "source": [
    "### Launch Jobs for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0337929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ads\n",
    "\n",
    "from ads.jobs import DataScienceJob\n",
    "from ads.jobs import DataScienceJobRun\n",
    "from ads.jobs import ScriptRuntime\n",
    "from ads.jobs import Job\n",
    "\n",
    "from ads import set_auth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04e981a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compartment_id = os.environ['NB_SESSION_COMPARTMENT_OCID']\n",
    "# project_id = os.environ['PROJECT_OCID']\n",
    "\n",
    "set_auth(auth='resource_principal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74d1d352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Here all the definitions\n",
    "#\n",
    "LOG_GROUP_ID = \"ocid1.loggroup.oc1.eu-frankfurt-1.amaaaaaangencdyazs4l4rzrzsarlej6mqlwlbz6bmnx4adwdlssveam2jaa\"\n",
    "LOG_ID = \"ocid1.log.oc1.eu-frankfurt-1.amaaaaaangencdya47httqmxyiew5tkxa6l7gekev2ljpasixuhmp2fa3v5q\"\n",
    "\n",
    "NAMESPACE = \"frqap2zhtzbe\"\n",
    "CONDA_BUCKET = \"whisper_jobs_env\"\n",
    "# bucket with code to execute\n",
    "SOURCE_BUCKET = \"whisper_jobs\"\n",
    "\n",
    "CUSTOM_ENV_URI = f\"oci://{CONDA_BUCKET}@{NAMESPACE}/conda_environments/gpu/whisper_env_/1.0/whisper_env_v1_0\"\n",
    "SOURCE_URI = f\"oci://{SOURCE_BUCKET}@{NAMESPACE}/test_atco2.tar.gz\"\n",
    "\n",
    "# the first to execute\n",
    "RUN_ENTRYPOINT = \"train.sh\"\n",
    "\n",
    "# SHAPE_NAME = \"VM.Standard2.4\"\n",
    "# SHAPE_NAME = \"VM.GPU2.1\"\n",
    "SHAPE_NAME = \"VM.GPU.A10.2\"\n",
    "# in GB\n",
    "STORAGE_SIZE = 2000\n",
    "\n",
    "JOBS_NAME = \"job_atco2_001\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c15d90c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Specify the Infrastructure requested\n",
    "# VM Shape, logging\n",
    "# network is taken from NB session\n",
    "\n",
    "# you need to provide the OCID for LogGroup and Log\n",
    "infrastructure = (\n",
    "    DataScienceJob()\n",
    "    .with_shape_name(SHAPE_NAME)\n",
    "    .with_block_storage_size(STORAGE_SIZE)\n",
    "    .with_log_group_id(LOG_GROUP_ID)\n",
    "    .with_log_id(LOG_ID)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a15eafea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the runtime and conda and env \n",
    "runtime = (\n",
    "    ScriptRuntime()\n",
    "    .with_source(SOURCE_URI)\n",
    "    .with_custom_conda(CUSTOM_ENV_URI)\n",
    "    .with_environment_variable(JOB_RUN_ENTRYPOINT=RUN_ENTRYPOINT)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fd6aed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the JOB\n",
    "job = (\n",
    "    Job(name=JOBS_NAME)\n",
    "    .with_infrastructure(infrastructure)\n",
    "    .with_runtime(runtime)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8f4cfb9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kind: job\n",
       "spec:\n",
       "  id: ocid1.datasciencejob.oc1.eu-frankfurt-1.amaaaaaangencdya5yd2lsefzidey4pkkyiq7rb6qav7j3wkshou42ay6pqq\n",
       "  infrastructure:\n",
       "    kind: infrastructure\n",
       "    spec:\n",
       "      blockStorageSize: 2000\n",
       "      compartmentId: ocid1.compartment.oc1..aaaaaaaag2cpni5qj6li5ny6ehuahhepbpveopobooayqfeudqygdtfe6h3a\n",
       "      displayName: job_atco2_001\n",
       "      jobInfrastructureType: STANDALONE\n",
       "      jobType: DEFAULT\n",
       "      logGroupId: ocid1.loggroup.oc1.eu-frankfurt-1.amaaaaaangencdyazs4l4rzrzsarlej6mqlwlbz6bmnx4adwdlssveam2jaa\n",
       "      logId: ocid1.log.oc1.eu-frankfurt-1.amaaaaaangencdya47httqmxyiew5tkxa6l7gekev2ljpasixuhmp2fa3v5q\n",
       "      projectId: ocid1.datascienceproject.oc1.eu-frankfurt-1.amaaaaaangencdyasybymsgwfmwo7ukyjs6kdl573kpxnb5rgy52c5irb5pq\n",
       "      shapeName: VM.GPU.A10.2\n",
       "      subnetId: ocid1.subnet.oc1.eu-frankfurt-1.aaaaaaaaijgqblnhpqle2zorl75qli23wre5eboqjtystagdgun4qwdxj4aq\n",
       "    type: dataScienceJob\n",
       "  name: job_atco2_001\n",
       "  runtime:\n",
       "    kind: runtime\n",
       "    spec:\n",
       "      conda:\n",
       "        type: published\n",
       "        uri: oci://whisper_jobs_env@frqap2zhtzbe/conda_environments/gpu/whisper_env_/1.0/whisper_env_v1_0\n",
       "      env:\n",
       "      - name: JOB_RUN_ENTRYPOINT\n",
       "        value: train.sh\n",
       "      scriptPathURI: oci://whisper_jobs@frqap2zhtzbe/test_atco2.tar.gz\n",
       "    type: script"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the JOB\n",
    "job.create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34594af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the job run by id\n",
    "# JOB_RUN_OCID = \"ocid1.datasciencejobrun.oc1.eu-frankfurt-1.amaaaaaangencdyais5w2gkzbqbwqeftps6raay65ymhg46hltw3vlokij3q\"\n",
    "\n",
    "# job_run = DataScienceJobRun.from_ocid(JOB_RUN_OCID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e4ac703",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_run = job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62647e81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job OCID: ocid1.datasciencejob.oc1.eu-frankfurt-1.amaaaaaangencdya5yd2lsefzidey4pkkyiq7rb6qav7j3wkshou42ay6pqq\n",
      "Job Run OCID: ocid1.datasciencejobrun.oc1.eu-frankfurt-1.amaaaaaangencdyahmu7llpebrusqkaoujcfwpu34mu5oziyfciayhjlnz5a\n",
      "2023-09-06 13:56:50 - Job Run ACCEPTED\n",
      "2023-09-06 13:56:57 - Job Run ACCEPTED, Infrastructure provisioning.\n",
      "2023-09-06 13:58:05 - Job Run ACCEPTED, Infrastructure provisioned.\n",
      "2023-09-06 13:58:46 - Job Run ACCEPTED, Job run bootstrap starting.\n",
      "2023-09-06 14:02:12 - Job Run ACCEPTED, Job run bootstrap complete. Artifact execution starting.\n",
      "2023-09-06 14:02:27 - Job Run IN_PROGRESS, Job run artifact execution in progress.\n",
      "2023-09-06 14:02:10 - *****************************************\n",
      "2023-09-06 14:02:10 - Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "2023-09-06 14:02:10 - *****************************************\n",
      "2023-09-06 14:02:10 - WARNING:torch.distributed.run:\n",
      "2023-09-06 14:02:10 - Fontconfig error: Cannot load default config file: No such file: (null)\n",
      "2023-09-06 14:02:10 - Fontconfig error: Cannot load default config file: No such file: (null)\n",
      "2023-09-06 14:02:14 - Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "2023-09-06 14:02:14 - Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "2023-09-06 14:02:14 - Login successful\n",
      "2023-09-06 14:02:14 - Your token has been saved to /home/datascience/.cache/huggingface/token\n",
      "2023-09-06 14:02:14 - Token is valid (permission: write).\n",
      "2023-09-06 14:02:14 - Token is valid (permission: write).\n",
      "2023-09-06 14:02:14 - Login successful\n",
      "2023-09-06 14:02:14 - Your token has been saved to /home/datascience/.cache/huggingface/token\n",
      "2023-09-06 14:02:14 - 09/06/2023 14:02:14 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "2023-09-06 14:02:14 - adam_epsilon=1e-08,\n",
      "2023-09-06 14:02:14 - adam_beta2=0.999,\n",
      "2023-09-06 14:02:14 - adam_beta1=0.9,\n",
      "2023-09-06 14:02:14 - adafactor=False,\n",
      "2023-09-06 14:02:14 - _n_gpu=1,\n",
      "2023-09-06 14:02:14 - 09/06/2023 14:02:14 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "2023-09-06 14:02:14 - dataloader_num_workers=0,\n",
      "2023-09-06 14:02:14 - dataloader_drop_last=False,\n",
      "2023-09-06 14:02:14 - data_seed=None,\n",
      "2023-09-06 14:02:14 - bf16_full_eval=False,\n",
      "2023-09-06 14:02:14 - bf16=False,\n",
      "2023-09-06 14:02:14 - auto_find_batch_size=False,\n",
      "2023-09-06 14:02:14 - ddp_timeout=1800,\n",
      "2023-09-06 14:02:14 - ddp_find_unused_parameters=None,\n",
      "2023-09-06 14:02:14 - ddp_bucket_cap_mb=None,\n",
      "2023-09-06 14:02:14 - ddp_backend=None,\n",
      "2023-09-06 14:02:14 - dataloader_pin_memory=True,\n",
      "2023-09-06 14:02:14 - eval_steps=50,\n",
      "2023-09-06 14:02:14 - eval_delay=0,\n",
      "2023-09-06 14:02:14 - eval_accumulation_steps=None,\n",
      "2023-09-06 14:02:14 - do_train=True,\n",
      "2023-09-06 14:02:14 - do_predict=False,\n",
      "2023-09-06 14:02:14 - do_eval=True,\n",
      "2023-09-06 14:02:14 - disable_tqdm=False,\n",
      "2023-09-06 14:02:14 - deepspeed=None,\n",
      "2023-09-06 14:02:14 - debug=[],\n",
      "2023-09-06 14:02:14 - fsdp_min_num_params=0,\n",
      "2023-09-06 14:02:14 - fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "2023-09-06 14:02:14 - fsdp=[],\n",
      "2023-09-06 14:02:14 - fp16_opt_level=O1,\n",
      "2023-09-06 14:02:14 - fp16_full_eval=False,\n",
      "2023-09-06 14:02:14 - fp16_backend=auto,\n",
      "2023-09-06 14:02:14 - fp16=True,\n",
      "2023-09-06 14:02:14 - evaluation_strategy=steps,\n",
      "2023-09-06 14:02:14 - generation_max_length=225,\n",
      "2023-09-06 14:02:14 - generation_config=None,\n",
      "2023-09-06 14:02:14 - full_determinism=False,\n",
      "2023-09-06 14:02:14 - fsdp_transformer_layer_cls_to_wrap=None,\n",
      "2023-09-06 14:02:14 - generation_num_beams=None,\n",
      "2023-09-06 14:02:14 - hub_strategy=every_save,\n",
      "2023-09-06 14:02:14 - hub_private_repo=False,\n",
      "2023-09-06 14:02:14 - hub_model_id=None,\n",
      "2023-09-06 14:02:14 - half_precision_backend=auto,\n",
      "2023-09-06 14:02:14 - group_by_length=False,\n",
      "2023-09-06 14:02:14 - greater_is_better=None,\n",
      "2023-09-06 14:02:14 - gradient_checkpointing=True,\n",
      "2023-09-06 14:02:14 - gradient_accumulation_steps=8,\n",
      "2023-09-06 14:02:14 - local_rank=0,\n",
      "2023-09-06 14:02:14 - load_best_model_at_end=False,\n",
      "2023-09-06 14:02:14 - length_column_name=input_length,\n",
      "2023-09-06 14:02:14 - learning_rate=1e-05,\n",
      "2023-09-06 14:02:14 - label_smoothing_factor=0.0,\n",
      "2023-09-06 14:02:14 - label_names=None,\n",
      "2023-09-06 14:02:14 - jit_mode_eval=False,\n",
      "2023-09-06 14:02:14 - include_inputs_for_metrics=False,\n",
      "2023-09-06 14:02:14 - ignore_data_skip=False,\n",
      "2023-09-06 14:02:14 - hub_token=<HUB_TOKEN>,\n",
      "2023-09-06 14:02:14 - max_grad_norm=1.0,\n",
      "2023-09-06 14:02:14 - lr_scheduler_type=linear,\n",
      "2023-09-06 14:02:14 - logging_strategy=steps,\n",
      "2023-09-06 14:02:14 - logging_steps=25,\n",
      "2023-09-06 14:02:14 - logging_nan_inf_filter=True,\n",
      "2023-09-06 14:02:14 - logging_first_step=False,\n",
      "2023-09-06 14:02:14 - logging_dir=/mnt/output/runs/Sep06_14-02-14_53029628aad2,\n",
      "2023-09-06 14:02:14 - log_on_each_node=True,\n",
      "2023-09-06 14:02:14 - log_level_replica=warning,\n",
      "2023-09-06 14:02:14 - log_level=passive,\n",
      "2023-09-06 14:02:14 - optim_args=None,\n",
      "2023-09-06 14:02:14 - optim=adamw_hf,\n",
      "2023-09-06 14:02:14 - num_train_epochs=3.0,\n",
      "2023-09-06 14:02:14 - no_cuda=False,\n",
      "2023-09-06 14:02:14 - mp_parameters=,\n",
      "2023-09-06 14:02:14 - metric_for_best_model=None,\n",
      "2023-09-06 14:02:14 - max_steps=1000,\n",
      "2023-09-06 14:02:14 - push_to_hub_organization=None,\n",
      "2023-09-06 14:02:14 - push_to_hub_model_id=None,\n",
      "2023-09-06 14:02:14 - push_to_hub=False,\n",
      "2023-09-06 14:02:14 - prediction_loss_only=False,\n",
      "2023-09-06 14:02:14 - predict_with_generate=True,\n",
      "2023-09-06 14:02:14 - per_device_train_batch_size=2,\n",
      "2023-09-06 14:02:14 - per_device_eval_batch_size=8,\n",
      "2023-09-06 14:02:14 - past_index=-1,\n",
      "2023-09-06 14:02:14 - overwrite_output_dir=True,\n",
      "2023-09-06 14:02:14 - output_dir=/mnt/output,\n",
      "2023-09-06 14:02:14 - save_steps=50,\n",
      "2023-09-06 14:02:14 - save_safetensors=False,\n",
      "2023-09-06 14:02:14 - save_on_each_node=False,\n",
      "2023-09-06 14:02:14 - run_name=/mnt/output,\n",
      "2023-09-06 14:02:14 - resume_from_checkpoint=None,\n",
      "2023-09-06 14:02:14 - report_to=['mlflow', 'tensorboard'],\n",
      "2023-09-06 14:02:14 - remove_unused_columns=True,\n",
      "2023-09-06 14:02:14 - ray_scope=last,\n",
      "2023-09-06 14:02:14 - push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "2023-09-06 14:02:14 - torch_compile_backend=None,\n",
      "2023-09-06 14:02:14 - torch_compile=False,\n",
      "2023-09-06 14:02:14 - tf32=None,\n",
      "2023-09-06 14:02:14 - sortish_sampler=False,\n",
      "2023-09-06 14:02:14 - skip_memory_metrics=True,\n",
      "2023-09-06 14:02:14 - sharded_ddp=[],\n",
      "2023-09-06 14:02:14 - seed=42,\n",
      "2023-09-06 14:02:14 - save_total_limit=None,\n",
      "2023-09-06 14:02:14 - save_strategy=steps,\n",
      "2023-09-06 14:02:14 - torch_compile_mode=None,\n",
      "2023-09-06 14:02:14 - xpu_backend=None,\n",
      "2023-09-06 14:02:14 - weight_decay=0.0,\n",
      "2023-09-06 14:02:14 - warmup_steps=200,\n",
      "2023-09-06 14:02:14 - warmup_ratio=0.0,\n",
      "2023-09-06 14:02:14 - use_mps_device=False,\n",
      "2023-09-06 14:02:14 - use_legacy_prediction_loop=False,\n",
      "2023-09-06 14:02:14 - use_ipex=False,\n",
      "2023-09-06 14:02:14 - tpu_num_cores=None,\n",
      "2023-09-06 14:02:14 - tpu_metrics_debug=False,\n",
      "2023-09-06 14:02:14 - torchdynamo=None,\n",
      "2023-09-06 14:02:14 - bf16_full_eval=False,\n",
      "2023-09-06 14:02:14 - bf16=False,\n",
      "2023-09-06 14:02:14 - auto_find_batch_size=False,\n",
      "2023-09-06 14:02:14 - adam_epsilon=1e-08,\n",
      "2023-09-06 14:02:14 - adam_beta2=0.999,\n",
      "2023-09-06 14:02:14 - adam_beta1=0.9,\n",
      "2023-09-06 14:02:14 - adafactor=False,\n",
      "2023-09-06 14:02:14 - _n_gpu=1,\n",
      "2023-09-06 14:02:14 - 09/06/2023 14:02:14 - INFO - __main__ - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
      "2023-09-06 14:02:14 - )\n",
      "2023-09-06 14:02:14 - ddp_find_unused_parameters=None,\n",
      "2023-09-06 14:02:14 - ddp_bucket_cap_mb=None,\n",
      "2023-09-06 14:02:14 - ddp_backend=None,\n",
      "2023-09-06 14:02:14 - dataloader_pin_memory=True,\n",
      "2023-09-06 14:02:14 - dataloader_num_workers=0,\n",
      "2023-09-06 14:02:14 - dataloader_drop_last=False,\n",
      "2023-09-06 14:02:14 - data_seed=None,\n",
      "2023-09-06 14:02:14 - eval_steps=50,\n",
      "2023-09-06 14:02:14 - eval_delay=0,\n",
      "2023-09-06 14:02:14 - eval_accumulation_steps=None,\n",
      "2023-09-06 14:02:14 - do_train=True,\n",
      "2023-09-06 14:02:14 - do_predict=False,\n",
      "2023-09-06 14:02:14 - do_eval=True,\n",
      "2023-09-06 14:02:14 - disable_tqdm=False,\n",
      "2023-09-06 14:02:14 - deepspeed=None,\n",
      "2023-09-06 14:02:14 - debug=[],\n",
      "2023-09-06 14:02:14 - ddp_timeout=1800,\n",
      "2023-09-06 14:02:14 - full_determinism=False,\n",
      "2023-09-06 14:02:14 - fsdp_transformer_layer_cls_to_wrap=None,\n",
      "2023-09-06 14:02:14 - fsdp_min_num_params=0,\n",
      "2023-09-06 14:02:14 - fsdp_config={'fsdp_min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
      "2023-09-06 14:02:14 - fsdp=[],\n",
      "2023-09-06 14:02:14 - fp16_opt_level=O1,\n",
      "2023-09-06 14:02:14 - fp16_full_eval=False,\n",
      "2023-09-06 14:02:14 - fp16_backend=auto,\n",
      "2023-09-06 14:02:14 - fp16=True,\n",
      "2023-09-06 14:02:14 - evaluation_strategy=steps,\n",
      "2023-09-06 14:02:14 - group_by_length=False,\n",
      "2023-09-06 14:02:14 - greater_is_better=None,\n",
      "2023-09-06 14:02:14 - gradient_checkpointing=True,\n",
      "2023-09-06 14:02:14 - gradient_accumulation_steps=8,\n",
      "2023-09-06 14:02:14 - generation_num_beams=None,\n",
      "2023-09-06 14:02:14 - generation_max_length=225,\n",
      "2023-09-06 14:02:14 - generation_config=None,\n",
      "2023-09-06 14:02:14 - label_smoothing_factor=0.0,\n",
      "2023-09-06 14:02:14 - label_names=None,\n",
      "2023-09-06 14:02:14 - jit_mode_eval=False,\n",
      "2023-09-06 14:02:14 - include_inputs_for_metrics=False,\n",
      "2023-09-06 14:02:14 - ignore_data_skip=False,\n",
      "2023-09-06 14:02:14 - hub_token=<HUB_TOKEN>,\n",
      "2023-09-06 14:02:14 - hub_strategy=every_save,\n",
      "2023-09-06 14:02:14 - hub_private_repo=False,\n",
      "2023-09-06 14:02:14 - hub_model_id=None,\n",
      "2023-09-06 14:02:14 - half_precision_backend=auto,\n",
      "2023-09-06 14:02:14 - logging_first_step=False,\n",
      "2023-09-06 14:02:14 - logging_dir=/mnt/output/runs/Sep06_14-02-14_53029628aad2,\n",
      "2023-09-06 14:02:14 - log_on_each_node=True,\n",
      "2023-09-06 14:02:14 - log_level_replica=warning,\n",
      "2023-09-06 14:02:14 - log_level=passive,\n",
      "2023-09-06 14:02:14 - local_rank=0,\n",
      "2023-09-06 14:02:14 - load_best_model_at_end=False,\n",
      "2023-09-06 14:02:14 - length_column_name=input_length,\n",
      "2023-09-06 14:02:14 - learning_rate=1e-05,\n",
      "2023-09-06 14:02:14 - mp_parameters=,\n",
      "2023-09-06 14:02:14 - metric_for_best_model=None,\n",
      "2023-09-06 14:02:14 - max_steps=1000,\n",
      "2023-09-06 14:02:14 - max_grad_norm=1.0,\n",
      "2023-09-06 14:02:14 - lr_scheduler_type=linear,\n",
      "2023-09-06 14:02:14 - logging_strategy=steps,\n",
      "2023-09-06 14:02:14 - logging_steps=25,\n",
      "2023-09-06 14:02:14 - logging_nan_inf_filter=True,\n",
      "2023-09-06 14:02:14 - predict_with_generate=True,\n",
      "2023-09-06 14:02:14 - per_device_train_batch_size=2,\n",
      "2023-09-06 14:02:14 - per_device_eval_batch_size=8,\n",
      "2023-09-06 14:02:14 - past_index=-1,\n",
      "2023-09-06 14:02:14 - overwrite_output_dir=True,\n",
      "2023-09-06 14:02:14 - output_dir=/mnt/output,\n",
      "2023-09-06 14:02:14 - optim_args=None,\n",
      "2023-09-06 14:02:14 - optim=adamw_hf,\n",
      "2023-09-06 14:02:14 - num_train_epochs=3.0,\n",
      "2023-09-06 14:02:14 - no_cuda=False,\n",
      "2023-09-06 14:02:14 - run_name=/mnt/output,\n",
      "2023-09-06 14:02:14 - resume_from_checkpoint=None,\n",
      "2023-09-06 14:02:14 - report_to=['mlflow', 'tensorboard'],\n",
      "2023-09-06 14:02:14 - remove_unused_columns=True,\n",
      "2023-09-06 14:02:14 - ray_scope=last,\n",
      "2023-09-06 14:02:14 - push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "2023-09-06 14:02:14 - push_to_hub_organization=None,\n",
      "2023-09-06 14:02:14 - push_to_hub_model_id=None,\n",
      "2023-09-06 14:02:14 - push_to_hub=False,\n",
      "2023-09-06 14:02:14 - prediction_loss_only=False,\n",
      "2023-09-06 14:02:14 - skip_memory_metrics=True,\n",
      "2023-09-06 14:02:14 - sharded_ddp=[],\n",
      "2023-09-06 14:02:14 - seed=42,\n",
      "2023-09-06 14:02:14 - save_total_limit=None,\n",
      "2023-09-06 14:02:14 - save_strategy=steps,\n",
      "2023-09-06 14:02:14 - save_steps=50,\n",
      "2023-09-06 14:02:14 - save_safetensors=False,\n",
      "2023-09-06 14:02:14 - save_on_each_node=False,\n",
      "2023-09-06 14:02:14 - tpu_num_cores=None,\n",
      "2023-09-06 14:02:14 - tpu_metrics_debug=False,\n",
      "2023-09-06 14:02:14 - torchdynamo=None,\n",
      "2023-09-06 14:02:14 - torch_compile_mode=None,\n",
      "2023-09-06 14:02:14 - torch_compile_backend=None,\n",
      "2023-09-06 14:02:14 - torch_compile=False,\n",
      "2023-09-06 14:02:14 - tf32=None,\n",
      "2023-09-06 14:02:14 - sortish_sampler=False,\n",
      "2023-09-06 14:02:14 - xpu_backend=None,\n",
      "2023-09-06 14:02:14 - weight_decay=0.0,\n",
      "2023-09-06 14:02:14 - warmup_steps=200,\n",
      "2023-09-06 14:02:14 - warmup_ratio=0.0,\n",
      "2023-09-06 14:02:14 - use_mps_device=False,\n",
      "2023-09-06 14:02:14 - use_legacy_prediction_loop=False,\n",
      "2023-09-06 14:02:14 - use_ipex=False,\n",
      "2023-09-06 14:02:14 - 09/06/2023 14:02:14 - WARNING - __main__ - Process rank: 1, device: cuda:1, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "2023-09-06 14:02:14 - )\n",
      "Downloading metadata: 100% 806/806 [00:00<00:00, 1.56MB/s]\n",
      "Downloading metadata: 100% 806/806 [00:00<00:00, 1.15MB/s]\n",
      "Downloading readme: 100% 220/220 [00:00<00:00, 299kB/s]\n",
      "2023-09-06 14:02:15 - Downloading and preparing dataset None/None (download: 120.09 MiB, generated: 121.04 MiB, post-processed: Unknown size, total: 241.14 MiB) to /home/datascience/.cache/huggingface/datasets/luigisaetta___parquet/luigisaetta--atco2-e21010af6311e730/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
      "Downloading data files:   0% 0/2 [00:00<?, ?it/s]\n",
      "Downloading data:   0% 0.00/113M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0% 52.2k/113M [00:00<06:28, 290kB/s]\u001b[A\n",
      "Downloading data:   0% 313k/113M [00:00<01:56, 967kB/s] \u001b[A\n",
      "Downloading data:   1% 1.34M/113M [00:00<00:35, 3.11MB/s]\u001b[A\n",
      "Downloading data:   3% 3.37M/113M [00:00<00:14, 7.65MB/s]\u001b[A\n",
      "Downloading data:   6% 7.31M/113M [00:00<00:06, 16.3MB/s]\u001b[A\n",
      "Downloading data:   9% 10.3M/113M [00:00<00:06, 16.3MB/s]\u001b[A\n",
      "Downloading data:  13% 14.1M/113M [00:01<00:04, 21.8MB/s]\u001b[A\n",
      "Downloading data:  16% 18.0M/113M [00:01<00:03, 26.1MB/s]\u001b[A\n",
      "Downloading data:  20% 22.0M/113M [00:01<00:03, 30.0MB/s]\u001b[A\n",
      "Downloading data:  23% 26.4M/113M [00:01<00:02, 33.8MB/s]\u001b[A\n",
      "Downloading data:  28% 31.0M/113M [00:01<00:02, 37.5MB/s]\u001b[A\n",
      "Downloading data:  32% 35.7M/113M [00:01<00:01, 40.1MB/s]\u001b[A\n",
      "Downloading data:  35% 39.8M/113M [00:01<00:02, 34.8MB/s]\u001b[A\n",
      "Downloading data:  39% 44.5M/113M [00:01<00:01, 37.9MB/s]\u001b[A\n",
      "Downloading data:  43% 48.6M/113M [00:01<00:01, 38.8MB/s]\u001b[A\n",
      "Downloading data:  47% 53.1M/113M [00:01<00:01, 40.5MB/s]\u001b[A\n",
      "Downloading data:  51% 57.8M/113M [00:02<00:01, 42.4MB/s]\u001b[A\n",
      "Downloading data:  55% 62.2M/113M [00:02<00:01, 35.2MB/s]\u001b[A\n",
      "Downloading data:  59% 66.4M/113M [00:02<00:01, 37.0MB/s]\u001b[A\n",
      "Downloading data:  63% 70.7M/113M [00:02<00:01, 38.5MB/s]\u001b[A\n",
      "Downloading data:  66% 74.7M/113M [00:02<00:01, 31.8MB/s]\u001b[A\n",
      "Downloading data:  70% 78.4M/113M [00:02<00:01, 33.2MB/s]\u001b[A\n",
      "Downloading data:  73% 82.0M/113M [00:02<00:01, 27.1MB/s]\u001b[A\n",
      "Downloading data:  75% 85.1M/113M [00:03<00:01, 24.5MB/s]\u001b[A\n",
      "Downloading data:  78% 88.0M/113M [00:03<00:00, 25.6MB/s]\u001b[A\n",
      "Downloading data:  81% 91.0M/113M [00:03<00:00, 26.5MB/s]\u001b[A\n",
      "Downloading data:  83% 93.8M/113M [00:03<00:00, 22.8MB/s]\u001b[A\n",
      "Downloading data:  85% 96.3M/113M [00:03<00:00, 19.2MB/s]\u001b[A\n",
      "Downloading data:  89% 100M/113M [00:03<00:00, 20.8MB/s] \u001b[A\n",
      "Downloading data:  91% 103M/113M [00:03<00:00, 21.1MB/s]\u001b[A\n",
      "Downloading data:  93% 105M/113M [00:04<00:00, 18.3MB/s]\u001b[A\n",
      "Downloading data:  95% 107M/113M [00:04<00:00, 19.6MB/s]\u001b[A\n",
      "Downloading data:  97% 109M/113M [00:04<00:00, 17.1MB/s]\u001b[A\n",
      "Downloading data: 100% 113M/113M [00:04<00:00, 24.7MB/s]\u001b[A\n",
      "Downloading data files:  50% 1/2 [00:05<00:05,  5.55s/it]\n",
      "Downloading data:   0% 0.00/13.2M [00:00<?, ?B/s]\u001b[A\n",
      "Downloading data:   0% 52.2k/13.2M [00:00<00:44, 292kB/s]\u001b[A\n",
      "Downloading data:   2% 313k/13.2M [00:00<00:13, 972kB/s] \u001b[A\n",
      "Downloading data:  10% 1.32M/13.2M [00:00<00:03, 3.48MB/s]\u001b[A\n",
      "Downloading data:  20% 2.70M/13.2M [00:00<00:01, 5.84MB/s]\u001b[A\n",
      "Downloading data:  48% 6.38M/13.2M [00:00<00:00, 14.3MB/s]\u001b[A\n",
      "Downloading data files: 100% 2/2 [00:07<00:00,  3.75s/it]\n",
      "Downloading data: 100% 13.2M/13.2M [00:00<00:00, 15.4MB/s]\u001b[A\n",
      "Extracting data files: 100% 2/2 [00:00<00:00, 2302.03it/s]\n",
      "2023-09-06 14:02:24 - Dataset parquet downloaded and prepared to /home/datascience/.cache/huggingface/datasets/luigisaetta___parquet/luigisaetta--atco2-e21010af6311e730/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
      "2023-09-06 14:02:24 - 09/06/2023 14:02:24 - WARNING - datasets.builder - Found cached dataset parquet (/home/datascience/.cache/huggingface/datasets/luigisaetta___parquet/luigisaetta--atco2-e21010af6311e730/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "2023-09-06 14:02:24 - 09/06/2023 14:02:24 - WARNING - datasets.builder - Found cached dataset parquet (/home/datascience/.cache/huggingface/datasets/luigisaetta___parquet/luigisaetta--atco2-e21010af6311e730/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "2023-09-06 14:02:25 - 09/06/2023 14:02:25 - WARNING - datasets.builder - Found cached dataset parquet (/home/datascience/.cache/huggingface/datasets/luigisaetta___parquet/luigisaetta--atco2-e21010af6311e730/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
      "Downloading (…)lve/main/config.json: 100% 1.99k/1.99k [00:00<00:00, 894kB/s]\n",
      "2023-09-06 14:02:25 - [INFO|configuration_utils.py:669] 2023-09-06 14:02:25,298 >> loading configuration file config.json from cache at /home/datascience/.cache/huggingface/hub/models--openai--whisper-medium/snapshots/edffa066cd39a0c5c58927735430ab5d630456a8/config.json\n",
      "2023-09-06 14:02:25 -   \"activation_dropout\": 0.0,\n",
      "2023-09-06 14:02:25 -   \"_name_or_path\": \"openai/whisper-medium\",\n",
      "2023-09-06 14:02:25 - [INFO|configuration_utils.py:725] 2023-09-06 14:02:25,302 >> Model config WhisperConfig {\n",
      "2023-09-06 14:02:25 -   ],\n",
      "2023-09-06 14:02:25 -     \"WhisperForConditionalGeneration\"\n",
      "2023-09-06 14:02:25 -   \"architectures\": [\n",
      "2023-09-06 14:02:25 -   \"apply_spec_augment\": false,\n",
      "2023-09-06 14:02:25 -   \"activation_function\": \"gelu\",\n",
      "2023-09-06 14:02:25 -   ],\n",
      "2023-09-06 14:02:25 -     50257\n",
      "2023-09-06 14:02:25 -     220,\n",
      "2023-09-06 14:02:25 -   \"begin_suppress_tokens\": [\n",
      "2023-09-06 14:02:25 -   \"attention_dropout\": 0.0,\n",
      "2023-09-06 14:02:25 -   \"decoder_start_token_id\": 50258,\n",
      "2023-09-06 14:02:25 -   \"decoder_layers\": 24,\n",
      "2023-09-06 14:02:25 -   \"decoder_layerdrop\": 0.0,\n",
      "2023-09-06 14:02:25 -   \"decoder_ffn_dim\": 4096,\n",
      "2023-09-06 14:02:25 -   \"decoder_attention_heads\": 16,\n",
      "2023-09-06 14:02:25 -   \"d_model\": 1024,\n",
      "2023-09-06 14:02:25 -   \"classifier_proj_size\": 256,\n",
      "2023-09-06 14:02:25 -   \"bos_token_id\": 50257,\n",
      "2023-09-06 14:02:25 -   \"forced_decoder_ids\": [\n",
      "2023-09-06 14:02:25 -   \"eos_token_id\": 50257,\n",
      "2023-09-06 14:02:25 -   \"encoder_layers\": 24,\n",
      "2023-09-06 14:02:25 -   \"encoder_layerdrop\": 0.0,\n",
      "2023-09-06 14:02:25 -   \"encoder_ffn_dim\": 4096,\n",
      "2023-09-06 14:02:25 -   \"encoder_attention_heads\": 16,\n",
      "2023-09-06 14:02:25 -   \"dropout\": 0.0,\n",
      "2023-09-06 14:02:25 -       50359\n",
      "2023-09-06 14:02:25 -       2,\n",
      "2023-09-06 14:02:25 -     [\n",
      "2023-09-06 14:02:25 -     ],\n",
      "2023-09-06 14:02:25 -       50259\n",
      "2023-09-06 14:02:25 -       1,\n",
      "2023-09-06 14:02:25 -     [\n",
      "2023-09-06 14:02:25 -   ],\n",
      "2023-09-06 14:02:25 -     ]\n",
      "2023-09-06 14:02:25 -       50363\n",
      "2023-09-06 14:02:25 -       3,\n",
      "2023-09-06 14:02:25 -     [\n",
      "2023-09-06 14:02:25 -     ],\n",
      "2023-09-06 14:02:25 -   \"mask_time_min_masks\": 2,\n",
      "2023-09-06 14:02:25 -   \"mask_time_length\": 10,\n",
      "2023-09-06 14:02:25 -   \"mask_feature_prob\": 0.0,\n",
      "2023-09-06 14:02:25 -   \"mask_feature_min_masks\": 0,\n",
      "2023-09-06 14:02:25 -   \"mask_feature_length\": 10,\n",
      "2023-09-06 14:02:25 -   \"is_encoder_decoder\": true,\n",
      "2023-09-06 14:02:25 -   \"init_std\": 0.02,\n",
      "2023-09-06 14:02:25 -   \"num_hidden_layers\": 24,\n",
      "2023-09-06 14:02:25 -   \"model_type\": \"whisper\",\n",
      "2023-09-06 14:02:25 -   \"max_target_positions\": 448,\n",
      "2023-09-06 14:02:25 -   \"max_source_positions\": 1500,\n",
      "2023-09-06 14:02:25 -   \"max_length\": 448,\n",
      "2023-09-06 14:02:25 -   \"mask_time_prob\": 0.05,\n",
      "2023-09-06 14:02:25 -     2,\n",
      "2023-09-06 14:02:25 -     1,\n",
      "2023-09-06 14:02:25 -   \"suppress_tokens\": [\n",
      "2023-09-06 14:02:25 -   \"scale_embedding\": false,\n",
      "2023-09-06 14:02:25 -   \"pad_token_id\": 50257,\n",
      "2023-09-06 14:02:25 -   \"num_mel_bins\": 80,\n",
      "2023-09-06 14:02:25 -     28,\n",
      "2023-09-06 14:02:25 -     27,\n",
      "2023-09-06 14:02:25 -     26,\n",
      "2023-09-06 14:02:25 -     25,\n",
      "2023-09-06 14:02:25 -     14,\n",
      "2023-09-06 14:02:25 -     10,\n",
      "2023-09-06 14:02:25 -     9,\n",
      "2023-09-06 14:02:25 -     8,\n",
      "2023-09-06 14:02:25 -     7,\n",
      "2023-09-06 14:02:25 -     60,\n",
      "2023-09-06 14:02:25 -     59,\n",
      "2023-09-06 14:02:25 -     58,\n",
      "2023-09-06 14:02:25 -     31,\n",
      "2023-09-06 14:02:25 -     29,\n",
      "2023-09-06 14:02:25 -     503,\n",
      "2023-09-06 14:02:25 -     359,\n",
      "2023-09-06 14:02:25 -     93,\n",
      "2023-09-06 14:02:25 -     92,\n",
      "2023-09-06 14:02:25 -     91,\n",
      "2023-09-06 14:02:25 -     90,\n",
      "2023-09-06 14:02:25 -     63,\n",
      "2023-09-06 14:02:25 -     62,\n",
      "2023-09-06 14:02:25 -     61,\n",
      "2023-09-06 14:02:25 -     918,\n",
      "2023-09-06 14:02:25 -     902,\n",
      "2023-09-06 14:02:25 -     893,\n",
      "2023-09-06 14:02:25 -     873,\n",
      "2023-09-06 14:02:25 -     542,\n",
      "2023-09-06 14:02:25 -     522,\n",
      "2023-09-06 14:02:25 -     2627,\n",
      "2023-09-06 14:02:25 -     2460,\n",
      "2023-09-06 14:02:25 -     1982,\n",
      "2023-09-06 14:02:25 -     1853,\n",
      "2023-09-06 14:02:25 -     1350,\n",
      "2023-09-06 14:02:25 -     931,\n",
      "2023-09-06 14:02:25 -     922,\n",
      "2023-09-06 14:02:25 -     6585,\n",
      "2023-09-06 14:02:25 -     4667,\n",
      "2023-09-06 14:02:25 -     4183,\n",
      "2023-09-06 14:02:25 -     3961,\n",
      "2023-09-06 14:02:25 -     3846,\n",
      "2023-09-06 14:02:25 -     3536,\n",
      "2023-09-06 14:02:25 -     3268,\n",
      "2023-09-06 14:02:25 -     3253,\n",
      "2023-09-06 14:02:25 -     3246,\n",
      "2023-09-06 14:02:25 -     10929,\n",
      "2023-09-06 14:02:25 -     10428,\n",
      "2023-09-06 14:02:25 -     9383,\n",
      "2023-09-06 14:02:25 -     9061,\n",
      "2023-09-06 14:02:25 -     7273,\n",
      "2023-09-06 14:02:25 -     6647,\n",
      "2023-09-06 14:02:25 -     14635,\n",
      "2023-09-06 14:02:25 -     14157,\n",
      "2023-09-06 14:02:25 -     13793,\n",
      "2023-09-06 14:02:25 -     12562,\n",
      "2023-09-06 14:02:25 -     12331,\n",
      "2023-09-06 14:02:25 -     12033,\n",
      "2023-09-06 14:02:25 -     11938,\n",
      "2023-09-06 14:02:25 -     20075,\n",
      "2023-09-06 14:02:25 -     18956,\n",
      "2023-09-06 14:02:25 -     18362,\n",
      "2023-09-06 14:02:25 -     16604,\n",
      "2023-09-06 14:02:25 -     16553,\n",
      "2023-09-06 14:02:25 -     15618,\n",
      "2023-09-06 14:02:25 -     15265,\n",
      "2023-09-06 14:02:25 -     29464,\n",
      "2023-09-06 14:02:25 -     28279,\n",
      "2023-09-06 14:02:25 -     26435,\n",
      "2023-09-06 14:02:25 -     26161,\n",
      "2023-09-06 14:02:25 -     26130,\n",
      "2023-09-06 14:02:25 -     22520,\n",
      "2023-09-06 14:02:25 -     21675,\n",
      "2023-09-06 14:02:25 -     47425,\n",
      "2023-09-06 14:02:25 -     42863,\n",
      "2023-09-06 14:02:25 -     36865,\n",
      "2023-09-06 14:02:25 -     32470,\n",
      "2023-09-06 14:02:25 -     32302,\n",
      "2023-09-06 14:02:25 -     31650,\n",
      "2023-09-06 14:02:25 -     50361,\n",
      "2023-09-06 14:02:25 -     50360,\n",
      "2023-09-06 14:02:25 -     50359,\n",
      "2023-09-06 14:02:25 -     50358,\n",
      "2023-09-06 14:02:25 -     50258,\n",
      "2023-09-06 14:02:25 -     50254,\n",
      "2023-09-06 14:02:25 -     49870,\n",
      "2023-09-06 14:02:25 - }\n",
      "2023-09-06 14:02:25 -   \"vocab_size\": 51865\n",
      "2023-09-06 14:02:25 -   \"use_weighted_layer_sum\": false,\n",
      "2023-09-06 14:02:25 -   \"use_cache\": true,\n",
      "2023-09-06 14:02:25 -   \"transformers_version\": \"4.30.2\",\n",
      "2023-09-06 14:02:25 -   \"torch_dtype\": \"float32\",\n",
      "2023-09-06 14:02:25 -   ],\n",
      "2023-09-06 14:02:25 -     50362\n",
      "2023-09-06 14:02:25 - \n",
      "Downloading (…)rocessor_config.json: 100% 185k/185k [00:00<00:00, 56.0MB/s]\n",
      "2023-09-06 14:02:25 - [INFO|feature_extraction_utils.py:477] 2023-09-06 14:02:25,464 >> loading configuration file preprocessor_config.json from cache at /home/datascience/.cache/huggingface/hub/models--openai--whisper-medium/snapshots/edffa066cd39a0c5c58927735430ab5d630456a8/preprocessor_config.json\n",
      "2023-09-06 14:02:25 -   \"n_fft\": 400,\n",
      "2023-09-06 14:02:25 -   \"hop_length\": 160,\n",
      "2023-09-06 14:02:25 -   \"feature_size\": 80,\n",
      "2023-09-06 14:02:25 -   \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
      "2023-09-06 14:02:25 -   \"chunk_length\": 30,\n",
      "2023-09-06 14:02:25 - [INFO|feature_extraction_utils.py:519] 2023-09-06 14:02:25,465 >> Feature extractor WhisperFeatureExtractor {\n",
      "2023-09-06 14:02:25 -   \"return_attention_mask\": false,\n",
      "2023-09-06 14:02:25 -   \"processor_class\": \"WhisperProcessor\",\n",
      "2023-09-06 14:02:25 -   \"padding_value\": 0.0,\n",
      "2023-09-06 14:02:25 -   \"padding_side\": \"right\",\n",
      "2023-09-06 14:02:25 -   \"nb_max_frames\": 3000,\n",
      "2023-09-06 14:02:25 -   \"n_samples\": 480000,\n",
      "2023-09-06 14:02:25 - \n",
      "2023-09-06 14:02:25 - }\n",
      "2023-09-06 14:02:25 -   \"sampling_rate\": 16000\n",
      "Downloading (…)okenizer_config.json: 100% 843/843 [00:00<00:00, 983kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100% 1.04M/1.04M [00:00<00:00, 71.5MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100% 2.20M/2.20M [00:00<00:00, 21.2MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100% 494k/494k [00:00<00:00, 56.6MB/s]\n",
      "Downloading (…)main/normalizer.json: 100% 52.7k/52.7k [00:00<00:00, 58.4MB/s]\n",
      "Downloading (…)in/added_tokens.json: 100% 2.08k/2.08k [00:00<00:00, 2.66MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100% 2.08k/2.08k [00:00<00:00, 3.09MB/s]\n",
      "2023-09-06 14:02:27 - [INFO|tokenization_utils_base.py:1823] 2023-09-06 14:02:27,131 >> loading file tokenizer.json from cache at /home/datascience/.cache/huggingface/hub/models--openai--whisper-medium/snapshots/edffa066cd39a0c5c58927735430ab5d630456a8/tokenizer.json\n",
      "2023-09-06 14:02:27 - [INFO|tokenization_utils_base.py:1823] 2023-09-06 14:02:27,131 >> loading file vocab.json from cache at /home/datascience/.cache/huggingface/hub/models--openai--whisper-medium/snapshots/edffa066cd39a0c5c58927735430ab5d630456a8/vocab.json\n",
      "2023-09-06 14:02:27 - [INFO|tokenization_utils_base.py:1823] 2023-09-06 14:02:27,131 >> loading file tokenizer_config.json from cache at /home/datascience/.cache/huggingface/hub/models--openai--whisper-medium/snapshots/edffa066cd39a0c5c58927735430ab5d630456a8/tokenizer_config.json\n",
      "2023-09-06 14:02:27 - [INFO|tokenization_utils_base.py:1823] 2023-09-06 14:02:27,131 >> loading file special_tokens_map.json from cache at /home/datascience/.cache/huggingface/hub/models--openai--whisper-medium/snapshots/edffa066cd39a0c5c58927735430ab5d630456a8/special_tokens_map.json\n",
      "2023-09-06 14:02:27 - [INFO|tokenization_utils_base.py:1823] 2023-09-06 14:02:27,131 >> loading file added_tokens.json from cache at /home/datascience/.cache/huggingface/hub/models--openai--whisper-medium/snapshots/edffa066cd39a0c5c58927735430ab5d630456a8/added_tokens.json\n",
      "2023-09-06 14:02:27 - [INFO|tokenization_utils_base.py:1823] 2023-09-06 14:02:27,131 >> loading file normalizer.json from cache at /home/datascience/.cache/huggingface/hub/models--openai--whisper-medium/snapshots/edffa066cd39a0c5c58927735430ab5d630456a8/normalizer.json\n",
      "2023-09-06 14:02:27 - [INFO|tokenization_utils_base.py:1823] 2023-09-06 14:02:27,131 >> loading file merges.txt from cache at /home/datascience/.cache/huggingface/hub/models--openai--whisper-medium/snapshots/edffa066cd39a0c5c58927735430ab5d630456a8/merges.txt\n",
      "Downloading model.safetensors: 100% 3.06G/3.06G [00:15<00:00, 199MB/s] \n",
      "2023-09-06 14:02:42 - [INFO|modeling_utils.py:2578] 2023-09-06 14:02:42,696 >> loading weights file model.safetensors from cache at /home/datascience/.cache/huggingface/hub/models--openai--whisper-medium/snapshots/edffa066cd39a0c5c58927735430ab5d630456a8/model.safetensors\n",
      "2023-09-06 14:02:42 -   \"_from_model_config\": true,\n",
      "2023-09-06 14:02:42 - [INFO|configuration_utils.py:577] 2023-09-06 14:02:42,827 >> Generate config GenerationConfig {\n",
      "2023-09-06 14:02:42 -   ],\n",
      "2023-09-06 14:02:42 -     50257\n",
      "2023-09-06 14:02:42 -     220,\n",
      "2023-09-06 14:02:42 -   \"begin_suppress_tokens\": [\n",
      "2023-09-06 14:02:42 -   \"transformers_version\": \"4.30.2\"\n",
      "2023-09-06 14:02:42 -   \"pad_token_id\": 50257,\n",
      "2023-09-06 14:02:42 -   \"max_length\": 448,\n",
      "2023-09-06 14:02:42 -   \"eos_token_id\": 50257,\n",
      "2023-09-06 14:02:42 -   \"decoder_start_token_id\": 50258,\n",
      "2023-09-06 14:02:42 -   \"bos_token_id\": 50257,\n",
      "2023-09-06 14:02:42 - \n",
      "2023-09-06 14:02:42 - }\n",
      "2023-09-06 14:02:49 - \n",
      "2023-09-06 14:02:49 - [INFO|modeling_utils.py:3295] 2023-09-06 14:02:49,690 >> All model checkpoint weights were used when initializing WhisperForConditionalGeneration.\n",
      "2023-09-06 14:02:49 - If your task is similar to the task the model of the checkpoint was trained on, you can already use WhisperForConditionalGeneration for predictions without further training.\n",
      "2023-09-06 14:02:49 - [INFO|modeling_utils.py:3303] 2023-09-06 14:02:49,690 >> All the weights of WhisperForConditionalGeneration were initialized from the model checkpoint at openai/whisper-medium.\n",
      "Downloading (…)neration_config.json: 100% 3.69k/3.69k [00:00<00:00, 1.90MB/s]\n",
      "2023-09-06 14:02:49 - [INFO|configuration_utils.py:539] 2023-09-06 14:02:49,903 >> loading configuration file generation_config.json from cache at /home/datascience/.cache/huggingface/hub/models--openai--whisper-medium/snapshots/edffa066cd39a0c5c58927735430ab5d630456a8/generation_config.json\n",
      "2023-09-06 14:02:49 -     [\n",
      "2023-09-06 14:02:49 -   \"alignment_heads\": [\n",
      "2023-09-06 14:02:49 - [INFO|configuration_utils.py:577] 2023-09-06 14:02:49,903 >> Generate config GenerationConfig {\n",
      "2023-09-06 14:02:49 -     [\n",
      "2023-09-06 14:02:49 -     ],\n",
      "2023-09-06 14:02:49 -       15\n",
      "2023-09-06 14:02:49 -       13,\n",
      "2023-09-06 14:02:49 -     ],\n",
      "2023-09-06 14:02:49 -       15\n",
      "2023-09-06 14:02:49 -       15,\n",
      "2023-09-06 14:02:49 -     [\n",
      "2023-09-06 14:02:49 -     ],\n",
      "2023-09-06 14:02:49 -       4\n",
      "2023-09-06 14:02:49 -       15,\n",
      "2023-09-06 14:02:49 -       0\n",
      "2023-09-06 14:02:49 -       20,\n",
      "2023-09-06 14:02:49 -     [\n",
      "2023-09-06 14:02:49 -     ],\n",
      "2023-09-06 14:02:49 -       1\n",
      "2023-09-06 14:02:49 -       16,\n",
      "2023-09-06 14:02:49 -     [\n",
      "2023-09-06 14:02:49 -   ],\n",
      "2023-09-06 14:02:49 -     ]\n",
      "2023-09-06 14:02:49 -       4\n",
      "2023-09-06 14:02:49 -       23,\n",
      "2023-09-06 14:02:49 -     [\n",
      "2023-09-06 14:02:49 -     ],\n",
      "2023-09-06 14:02:49 -   \"forced_decoder_ids\": [\n",
      "2023-09-06 14:02:49 -   \"eos_token_id\": 50257,\n",
      "2023-09-06 14:02:49 -   \"decoder_start_token_id\": 50258,\n",
      "2023-09-06 14:02:49 -   \"bos_token_id\": 50257,\n",
      "2023-09-06 14:02:49 -   ],\n",
      "2023-09-06 14:02:49 -     50257\n",
      "2023-09-06 14:02:49 -     220,\n",
      "2023-09-06 14:02:49 -   \"begin_suppress_tokens\": [\n",
      "2023-09-06 14:02:49 -     ]\n",
      "2023-09-06 14:02:49 -       50359\n",
      "2023-09-06 14:02:49 -       2,\n",
      "2023-09-06 14:02:49 -     [\n",
      "2023-09-06 14:02:49 -     ],\n",
      "2023-09-06 14:02:49 -       null\n",
      "2023-09-06 14:02:49 -       1,\n",
      "2023-09-06 14:02:49 -     [\n",
      "2023-09-06 14:02:49 -     \"<|as|>\": 50350,\n",
      "2023-09-06 14:02:49 -     \"<|ar|>\": 50272,\n",
      "2023-09-06 14:02:49 -     \"<|am|>\": 50334,\n",
      "2023-09-06 14:02:49 -     \"<|af|>\": 50327,\n",
      "2023-09-06 14:02:49 -   \"lang_to_id\": {\n",
      "2023-09-06 14:02:49 -   \"is_multilingual\": true,\n",
      "2023-09-06 14:02:49 -   ],\n",
      "2023-09-06 14:02:49 -     \"<|bo|>\": 50347,\n",
      "2023-09-06 14:02:49 -     \"<|bn|>\": 50302,\n",
      "2023-09-06 14:02:49 -     \"<|bg|>\": 50292,\n",
      "2023-09-06 14:02:49 -     \"<|be|>\": 50330,\n",
      "2023-09-06 14:02:49 -     \"<|ba|>\": 50355,\n",
      "2023-09-06 14:02:49 -     \"<|az|>\": 50304,\n",
      "2023-09-06 14:02:49 -     \"<|da|>\": 50285,\n",
      "2023-09-06 14:02:49 -     \"<|cy|>\": 50297,\n",
      "2023-09-06 14:02:49 -     \"<|cs|>\": 50283,\n",
      "2023-09-06 14:02:49 -     \"<|ca|>\": 50270,\n",
      "2023-09-06 14:02:49 -     \"<|bs|>\": 50315,\n",
      "2023-09-06 14:02:49 -     \"<|br|>\": 50309,\n",
      "2023-09-06 14:02:49 -     \"<|fi|>\": 50277,\n",
      "2023-09-06 14:02:49 -     \"<|fa|>\": 50300,\n",
      "2023-09-06 14:02:49 -     \"<|eu|>\": 50310,\n",
      "2023-09-06 14:02:49 -     \"<|et|>\": 50307,\n",
      "2023-09-06 14:02:49 -     \"<|es|>\": 50262,\n",
      "2023-09-06 14:02:49 -     \"<|en|>\": 50259,\n",
      "2023-09-06 14:02:49 -     \"<|el|>\": 50281,\n",
      "2023-09-06 14:02:49 -     \"<|de|>\": 50261,\n",
      "2023-09-06 14:02:49 -     \"<|haw|>\": 50352,\n",
      "2023-09-06 14:02:49 -     \"<|gu|>\": 50333,\n",
      "2023-09-06 14:02:49 -     \"<|gl|>\": 50319,\n",
      "2023-09-06 14:02:49 -     \"<|fr|>\": 50265,\n",
      "2023-09-06 14:02:49 -     \"<|fo|>\": 50338,\n",
      "2023-09-06 14:02:49 -     \"<|hy|>\": 50312,\n",
      "2023-09-06 14:02:49 -     \"<|hu|>\": 50286,\n",
      "2023-09-06 14:02:49 -     \"<|ht|>\": 50339,\n",
      "2023-09-06 14:02:49 -     \"<|hr|>\": 50291,\n",
      "2023-09-06 14:02:49 -     \"<|hi|>\": 50276,\n",
      "2023-09-06 14:02:49 -     \"<|he|>\": 50279,\n",
      "2023-09-06 14:02:49 -     \"<|ha|>\": 50354,\n",
      "2023-09-06 14:02:49 -     \"<|km|>\": 50323,\n",
      "2023-09-06 14:02:49 -     \"<|kk|>\": 50316,\n",
      "2023-09-06 14:02:49 -     \"<|ka|>\": 50329,\n",
      "2023-09-06 14:02:49 -     \"<|jw|>\": 50356,\n",
      "2023-09-06 14:02:49 -     \"<|ja|>\": 50266,\n",
      "2023-09-06 14:02:49 -     \"<|it|>\": 50274,\n",
      "2023-09-06 14:02:49 -     \"<|is|>\": 50311,\n",
      "2023-09-06 14:02:49 -     \"<|id|>\": 50275,\n",
      "2023-09-06 14:02:49 -     \"<|ln|>\": 50353,\n",
      "2023-09-06 14:02:49 -     \"<|lb|>\": 50345,\n",
      "2023-09-06 14:02:49 -     \"<|la|>\": 50294,\n",
      "2023-09-06 14:02:49 -     \"<|ko|>\": 50264,\n",
      "2023-09-06 14:02:49 -     \"<|kn|>\": 50306,\n",
      "2023-09-06 14:02:49 -     \"<|ml|>\": 50296,\n",
      "2023-09-06 14:02:49 -     \"<|mk|>\": 50308,\n",
      "2023-09-06 14:02:49 -     \"<|mi|>\": 50295,\n",
      "2023-09-06 14:02:49 -     \"<|mg|>\": 50349,\n",
      "2023-09-06 14:02:49 -     \"<|lv|>\": 50301,\n",
      "2023-09-06 14:02:49 -     \"<|lt|>\": 50293,\n",
      "2023-09-06 14:02:49 -     \"<|lo|>\": 50336,\n",
      "2023-09-06 14:02:49 -     \"<|nl|>\": 50271,\n",
      "2023-09-06 14:02:49 -     \"<|ne|>\": 50313,\n",
      "2023-09-06 14:02:49 -     \"<|my|>\": 50346,\n",
      "2023-09-06 14:02:49 -     \"<|mt|>\": 50343,\n",
      "2023-09-06 14:02:49 -     \"<|ms|>\": 50282,\n",
      "2023-09-06 14:02:49 -     \"<|mr|>\": 50320,\n",
      "2023-09-06 14:02:49 -     \"<|mn|>\": 50314,\n",
      "2023-09-06 14:02:49 -     \"<|pl|>\": 50269,\n",
      "2023-09-06 14:02:49 -     \"<|pa|>\": 50321,\n",
      "2023-09-06 14:02:49 -     \"<|oc|>\": 50328,\n",
      "2023-09-06 14:02:49 -     \"<|no|>\": 50288,\n",
      "2023-09-06 14:02:49 -     \"<|nn|>\": 50342,\n",
      "2023-09-06 14:02:49 -     \"<|sk|>\": 50298,\n",
      "2023-09-06 14:02:49 -     \"<|si|>\": 50322,\n",
      "2023-09-06 14:02:49 -     \"<|sd|>\": 50332,\n",
      "2023-09-06 14:02:49 -     \"<|sa|>\": 50344,\n",
      "2023-09-06 14:02:49 -     \"<|ru|>\": 50263,\n",
      "2023-09-06 14:02:49 -     \"<|ro|>\": 50284,\n",
      "2023-09-06 14:02:49 -     \"<|pt|>\": 50267,\n",
      "2023-09-06 14:02:49 -     \"<|ps|>\": 50340,\n",
      "2023-09-06 14:02:49 -     \"<|sv|>\": 50273,\n",
      "2023-09-06 14:02:49 -     \"<|su|>\": 50357,\n",
      "2023-09-06 14:02:49 -     \"<|sr|>\": 50303,\n",
      "2023-09-06 14:02:49 -     \"<|sq|>\": 50317,\n",
      "2023-09-06 14:02:49 -     \"<|so|>\": 50326,\n",
      "2023-09-06 14:02:49 -     \"<|sn|>\": 50324,\n",
      "2023-09-06 14:02:49 -     \"<|sl|>\": 50305,\n",
      "2023-09-06 14:02:49 -     \"<|tr|>\": 50268,\n",
      "2023-09-06 14:02:49 -     \"<|tl|>\": 50348,\n",
      "2023-09-06 14:02:49 -     \"<|tk|>\": 50341,\n",
      "2023-09-06 14:02:49 -     \"<|th|>\": 50289,\n",
      "2023-09-06 14:02:49 -     \"<|tg|>\": 50331,\n",
      "2023-09-06 14:02:49 -     \"<|te|>\": 50299,\n",
      "2023-09-06 14:02:49 -     \"<|ta|>\": 50287,\n",
      "2023-09-06 14:02:49 -     \"<|sw|>\": 50318,\n",
      "2023-09-06 14:02:49 -     \"<|yo|>\": 50325,\n",
      "2023-09-06 14:02:49 -     \"<|yi|>\": 50335,\n",
      "2023-09-06 14:02:49 -     \"<|vi|>\": 50278,\n",
      "2023-09-06 14:02:49 -     \"<|uz|>\": 50337,\n",
      "2023-09-06 14:02:49 -     \"<|ur|>\": 50290,\n",
      "2023-09-06 14:02:49 -     \"<|uk|>\": 50280,\n",
      "2023-09-06 14:02:49 -     \"<|tt|>\": 50351,\n",
      "2023-09-06 14:02:49 -     1,\n",
      "2023-09-06 14:02:49 -   \"suppress_tokens\": [\n",
      "2023-09-06 14:02:49 -   \"pad_token_id\": 50257,\n",
      "2023-09-06 14:02:49 -   \"no_timestamps_token_id\": 50363,\n",
      "2023-09-06 14:02:49 -   \"max_length\": 448,\n",
      "2023-09-06 14:02:49 -   \"max_initial_timestamp_index\": 1,\n",
      "2023-09-06 14:02:49 -   },\n",
      "2023-09-06 14:02:49 -     \"<|zh|>\": 50260\n",
      "2023-09-06 14:02:49 -     25,\n",
      "2023-09-06 14:02:49 -     14,\n",
      "2023-09-06 14:02:49 -     10,\n",
      "2023-09-06 14:02:49 -     9,\n",
      "2023-09-06 14:02:49 -     8,\n",
      "2023-09-06 14:02:49 -     7,\n",
      "2023-09-06 14:02:49 -     2,\n",
      "2023-09-06 14:02:49 -     60,\n",
      "2023-09-06 14:02:49 -     59,\n",
      "2023-09-06 14:02:49 -     58,\n",
      "2023-09-06 14:02:49 -     31,\n",
      "2023-09-06 14:02:49 -     29,\n",
      "2023-09-06 14:02:49 -     28,\n",
      "2023-09-06 14:02:49 -     27,\n",
      "2023-09-06 14:02:49 -     26,\n",
      "2023-09-06 14:02:49 -     91,\n",
      "2023-09-06 14:02:49 -     90,\n",
      "2023-09-06 14:02:49 -     63,\n",
      "2023-09-06 14:02:49 -     62,\n",
      "2023-09-06 14:02:49 -     61,\n",
      "2023-09-06 14:02:49 -     873,\n",
      "2023-09-06 14:02:49 -     542,\n",
      "2023-09-06 14:02:49 -     522,\n",
      "2023-09-06 14:02:49 -     503,\n",
      "2023-09-06 14:02:49 -     359,\n",
      "2023-09-06 14:02:49 -     93,\n",
      "2023-09-06 14:02:49 -     92,\n",
      "2023-09-06 14:02:49 -     1853,\n",
      "2023-09-06 14:02:49 -     1350,\n",
      "2023-09-06 14:02:49 -     931,\n",
      "2023-09-06 14:02:49 -     922,\n",
      "2023-09-06 14:02:49 -     918,\n",
      "2023-09-06 14:02:49 -     902,\n",
      "2023-09-06 14:02:49 -     893,\n",
      "2023-09-06 14:02:49 -     3846,\n",
      "2023-09-06 14:02:49 -     3536,\n",
      "2023-09-06 14:02:49 -     3268,\n",
      "2023-09-06 14:02:49 -     3253,\n",
      "2023-09-06 14:02:49 -     3246,\n",
      "2023-09-06 14:02:49 -     2627,\n",
      "2023-09-06 14:02:49 -     2460,\n",
      "2023-09-06 14:02:49 -     1982,\n",
      "2023-09-06 14:02:49 -     9061,\n",
      "2023-09-06 14:02:49 -     7273,\n",
      "2023-09-06 14:02:49 -     6647,\n",
      "2023-09-06 14:02:49 -     6585,\n",
      "2023-09-06 14:02:49 -     4667,\n",
      "2023-09-06 14:02:49 -     4183,\n",
      "2023-09-06 14:02:49 -     3961,\n",
      "2023-09-06 14:02:49 -     12562,\n",
      "2023-09-06 14:02:49 -     12331,\n",
      "2023-09-06 14:02:49 -     12033,\n",
      "2023-09-06 14:02:49 -     11938,\n",
      "2023-09-06 14:02:49 -     10929,\n",
      "2023-09-06 14:02:49 -     10428,\n",
      "2023-09-06 14:02:49 -     9383,\n",
      "2023-09-06 14:02:49 -     18362,\n",
      "2023-09-06 14:02:49 -     16604,\n",
      "2023-09-06 14:02:49 -     16553,\n",
      "2023-09-06 14:02:49 -     15618,\n",
      "2023-09-06 14:02:49 -     15265,\n",
      "2023-09-06 14:02:49 -     14635,\n",
      "2023-09-06 14:02:49 -     14157,\n",
      "2023-09-06 14:02:49 -     13793,\n",
      "2023-09-06 14:02:49 -     26435,\n",
      "2023-09-06 14:02:49 -     26161,\n",
      "2023-09-06 14:02:49 -     26130,\n",
      "2023-09-06 14:02:49 -     22520,\n",
      "2023-09-06 14:02:49 -     21675,\n",
      "2023-09-06 14:02:49 -     20075,\n",
      "2023-09-06 14:02:49 -     18956,\n",
      "2023-09-06 14:02:49 -     47425,\n",
      "2023-09-06 14:02:49 -     42863,\n",
      "2023-09-06 14:02:49 -     36865,\n",
      "2023-09-06 14:02:49 -     32470,\n",
      "2023-09-06 14:02:49 -     32302,\n",
      "2023-09-06 14:02:49 -     31650,\n",
      "2023-09-06 14:02:49 -     29464,\n",
      "2023-09-06 14:02:49 -     28279,\n",
      "2023-09-06 14:02:49 -     50359,\n",
      "2023-09-06 14:02:49 -     50358,\n",
      "2023-09-06 14:02:49 -     50258,\n",
      "2023-09-06 14:02:49 -     50254,\n",
      "2023-09-06 14:02:49 -     49870,\n",
      "2023-09-06 14:02:49 -     \"translate\": 50358\n",
      "2023-09-06 14:02:49 -     \"transcribe\": 50359,\n",
      "2023-09-06 14:02:49 -   \"task_to_id\": {\n",
      "2023-09-06 14:02:49 -   ],\n",
      "2023-09-06 14:02:49 -     50362\n",
      "2023-09-06 14:02:49 -     50361,\n",
      "2023-09-06 14:02:49 -     50360,\n",
      "2023-09-06 14:02:49 - \n",
      "2023-09-06 14:02:49 - }\n",
      "2023-09-06 14:02:49 -   \"transformers_version\": \"4.30.2\"\n",
      "2023-09-06 14:02:49 -   },\n",
      "2023-09-06 14:03:12 - 09/06/2023 14:03:12 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/datascience/.cache/huggingface/datasets/luigisaetta___parquet/luigisaetta--atco2-e21010af6311e730/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-284f2078bb933bb9.arrow\n",
      "2023-09-06 14:03:12 - 09/06/2023 14:03:12 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/datascience/.cache/huggingface/datasets/luigisaetta___parquet/luigisaetta--atco2-e21010af6311e730/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-275d64deb6b64785.arrow\n",
      "2023-09-06 14:03:12 - 09/06/2023 14:03:12 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/datascience/.cache/huggingface/datasets/luigisaetta___parquet/luigisaetta--atco2-e21010af6311e730/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-a9ad010831b354da.arrow\n",
      "2023-09-06 14:03:12 - 09/06/2023 14:03:12 - WARNING - datasets.arrow_dataset - Loading cached processed dataset at /home/datascience/.cache/huggingface/datasets/luigisaetta___parquet/luigisaetta--atco2-e21010af6311e730/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-765a093aad63ba95.arrow\n",
      "Downloading builder script: 100% 4.49k/4.49k [00:00<00:00, 7.38MB/s]  \n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils_base.py:2201] 2023-09-06 14:03:13,157 >> Special tokens file saved in /mnt/output/special_tokens_map.json\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils_base.py:2194] 2023-09-06 14:03:13,156 >> tokenizer config file saved in /mnt/output/tokenizer_config.json\n",
      "2023-09-06 14:03:13 - [INFO|feature_extraction_utils.py:377] 2023-09-06 14:03:13,156 >> Feature extractor saved in /mnt/output/preprocessor_config.json\n",
      "2023-09-06 14:03:13 - [INFO|configuration_utils.py:458] 2023-09-06 14:03:13,260 >> Configuration saved in /mnt/output/config.json\n",
      "Downloading builder script: 100% 4.49k/4.49k [00:00<00:00, 3.87MB/s]\n",
      "2023-09-06 14:03:13 - [INFO|image_processing_utils.py:307] 2023-09-06 14:03:13,421 >> loading configuration file /mnt/output/preprocessor_config.json\n",
      "2023-09-06 14:03:13 - [INFO|feature_extraction_utils.py:475] 2023-09-06 14:03:13,429 >> loading configuration file /mnt/output/preprocessor_config.json\n",
      "2023-09-06 14:03:13 -   \"feature_extractor_type\": \"WhisperFeatureExtractor\",\n",
      "2023-09-06 14:03:13 -   \"chunk_length\": 30,\n",
      "2023-09-06 14:03:13 - [INFO|feature_extraction_utils.py:519] 2023-09-06 14:03:13,429 >> Feature extractor WhisperFeatureExtractor {\n",
      "2023-09-06 14:03:13 -   \"n_fft\": 400,\n",
      "2023-09-06 14:03:13 -   \"hop_length\": 160,\n",
      "2023-09-06 14:03:13 -   \"feature_size\": 80,\n",
      "2023-09-06 14:03:13 -   \"return_attention_mask\": false,\n",
      "2023-09-06 14:03:13 -   \"processor_class\": \"WhisperProcessor\",\n",
      "2023-09-06 14:03:13 -   \"padding_value\": 0.0,\n",
      "2023-09-06 14:03:13 -   \"padding_side\": \"right\",\n",
      "2023-09-06 14:03:13 -   \"nb_max_frames\": 3000,\n",
      "2023-09-06 14:03:13 -   \"n_samples\": 480000,\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils_base.py:1821] 2023-09-06 14:03:13,430 >> loading file merges.txt\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils_base.py:1821] 2023-09-06 14:03:13,430 >> loading file tokenizer.json\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils_base.py:1821] 2023-09-06 14:03:13,430 >> loading file vocab.json\n",
      "2023-09-06 14:03:13 - \n",
      "2023-09-06 14:03:13 - }\n",
      "2023-09-06 14:03:13 -   \"sampling_rate\": 16000\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils_base.py:1821] 2023-09-06 14:03:13,430 >> loading file normalizer.json\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils_base.py:1821] 2023-09-06 14:03:13,430 >> loading file tokenizer_config.json\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils_base.py:1821] 2023-09-06 14:03:13,430 >> loading file special_tokens_map.json\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils_base.py:1821] 2023-09-06 14:03:13,430 >> loading file added_tokens.json\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,488 >> Adding <|en|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,488 >> Adding <|startoftranscript|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,488 >> Adding <|fr|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,488 >> Adding <|ko|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,488 >> Adding <|ru|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,488 >> Adding <|es|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,488 >> Adding <|de|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,488 >> Adding <|zh|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|ca|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|pl|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|tr|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,488 >> Adding <|pt|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,488 >> Adding <|ja|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|fi|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|hi|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|id|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|it|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|sv|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|ar|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|nl|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|ro|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|cs|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|ms|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|el|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|uk|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|he|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|vi|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|hr|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|ur|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|th|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|no|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|ta|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|hu|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|da|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|ml|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|mi|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|la|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|lt|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|bg|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|sr|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|bn|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|lv|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|fa|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|te|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|sk|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|cy|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|is|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|eu|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|br|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|mk|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|et|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|kn|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|sl|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,489 >> Adding <|az|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|ne|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|hy|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|tg|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|be|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|ka|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|oc|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|af|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|so|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|yo|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|sn|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|km|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|si|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|pa|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|mr|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|gl|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|sw|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|sq|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|kk|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|bs|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|mn|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|lo|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|yi|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|am|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|gu|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|sd|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|mt|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|nn|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|tk|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|ps|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|ht|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|fo|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|uz|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,491 >> Adding <|tt|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,491 >> Adding <|as|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,491 >> Adding <|mg|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,491 >> Adding <|tl|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,491 >> Adding <|bo|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,491 >> Adding <|my|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,491 >> Adding <|lb|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,490 >> Adding <|sa|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,491 >> Adding <|translate|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,491 >> Adding <|su|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,491 >> Adding <|jw|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,491 >> Adding <|ba|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,491 >> Adding <|ha|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,491 >> Adding <|ln|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,491 >> Adding <|haw|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,491 >> Adding <|notimestamps|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,491 >> Adding <|nocaptions|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,491 >> Adding <|startofprev|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,491 >> Adding <|startoflm|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|tokenization_utils.py:426] 2023-09-06 14:03:13,491 >> Adding <|transcribe|> to the vocabulary\n",
      "2023-09-06 14:03:13 - [INFO|trainer.py:577] 2023-09-06 14:03:13,848 >> max_steps is given, it will override any value given in num_train_epochs\n",
      "2023-09-06 14:03:14 - \n",
      "2023-09-06 14:03:14 -   warnings.warn(\n",
      "2023-09-06 14:03:14 - 09/06/2023 14:03:14 - WARNING - py.warnings - /home/datascience/conda/whisper_env_v1_0/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "2023-09-06 14:03:14 - [INFO|trainer.py:776] 2023-09-06 14:03:14,013 >> The following columns in the training set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: input_length. If input_length are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "2023-09-06 14:03:14 - \n",
      "2023-09-06 14:03:14 -   warnings.warn(\n",
      "2023-09-06 14:03:14 - 09/06/2023 14:03:14 - WARNING - py.warnings - /home/datascience/conda/whisper_env_v1_0/lib/python3.8/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "2023-09-06 14:03:14 - [INFO|trainer.py:1787] 2023-09-06 14:03:14,091 >>   Num examples = 504\n",
      "2023-09-06 14:03:14 - [INFO|trainer.py:1786] 2023-09-06 14:03:14,091 >> ***** Running training *****\n",
      "2023-09-06 14:03:14 - [INFO|trainer.py:1792] 2023-09-06 14:03:14,091 >>   Total optimization steps = 1,000\n",
      "2023-09-06 14:03:14 - [INFO|trainer.py:1791] 2023-09-06 14:03:14,091 >>   Gradient Accumulation steps = 8\n",
      "2023-09-06 14:03:14 - [INFO|trainer.py:1790] 2023-09-06 14:03:14,091 >>   Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "2023-09-06 14:03:14 - [INFO|trainer.py:1789] 2023-09-06 14:03:14,091 >>   Instantaneous batch size per device = 2\n",
      "2023-09-06 14:03:14 - [INFO|trainer.py:1788] 2023-09-06 14:03:14,091 >>   Num Epochs = 67\n",
      "2023-09-06 14:03:14 - [INFO|trainer.py:1793] 2023-09-06 14:03:14,093 >>   Number of trainable parameters = 763,857,920\n",
      "2023-09-06 14:03:14 - To disable this warning, you can either:\n",
      "2023-09-06 14:03:14 - huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "2023-09-06 14:03:14 - \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2023-09-06 14:03:14 - \t- Avoid using `tokenizers` before the fork if possible\n",
      "2023-09-06 14:03:14 - huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "2023-09-06 14:03:14 - \t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "2023-09-06 14:03:14 - \t- Avoid using `tokenizers` before the fork if possible\n",
      "2023-09-06 14:03:14 - To disable this warning, you can either:\n",
      "  0% 0/1000 [00:00<?, ?it/s][WARNING|logging.py:295] 2023-09-06 14:03:15,580 >> `use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "2023-09-06 14:03:15 - [WARNING|logging.py:295] 2023-09-06 14:03:15,755 >> `use_cache = True` is incompatible with gradient checkpointing. Setting `use_cache = False`...\n",
      "2023-09-06 14:06:07 - {'loss': 2.5616, 'learning_rate': 1.2000000000000002e-06, 'epoch': 1.59}\n",
      "2023-09-06 14:08:59 - {'loss': 1.3051, 'learning_rate': 2.4000000000000003e-06, 'epoch': 3.17}\n",
      "2023-09-06 14:09:30 - {'eval_loss': 0.9045917391777039, 'eval_wer': 0.5461465271170314, 'eval_runtime': 30.3311, 'eval_samples_per_second': 1.813, 'eval_steps_per_second': 0.132, 'epoch': 3.17}\n",
      "2023-09-06 14:08:59 - [INFO|trainer.py:3200] 2023-09-06 14:08:59,909 >> ***** Running Evaluation *****\n",
      "  5% 50/1000 [05:45<1:49:33,  6.92s/it][INFO|trainer.py:776] 2023-09-06 14:08:59,907 >> The following columns in the evaluation set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: input_length. If input_length are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "2023-09-06 14:08:59 - [INFO|trainer.py:3205] 2023-09-06 14:08:59,909 >>   Batch size = 8\n",
      "2023-09-06 14:08:59 - [INFO|trainer.py:3202] 2023-09-06 14:08:59,909 >>   Num examples = 55\n",
      "2023-09-06 14:09:08 - \n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:07<00:07,  3.67s/it]\u001b[A\n",
      " 75% 3/4 [00:14<00:05,  5.19s/it]\u001b[A\n",
      "                                       \n",
      "                                 \u001b[A[INFO|trainer.py:2926] 2023-09-06 14:09:30,243 >> Saving model checkpoint to /mnt/output/checkpoint-50\n",
      "100% 4/4 [00:22<00:00,  5.97s/it]\u001b[A\n",
      "  5% 50/1000 [06:15<1:49:33,  6.92s/it]\n",
      "2023-09-06 14:09:30 - [INFO|configuration_utils.py:364] 2023-09-06 14:09:30,245 >> Configuration saved in /mnt/output/checkpoint-50/generation_config.json\n",
      "2023-09-06 14:09:30 - [INFO|configuration_utils.py:458] 2023-09-06 14:09:30,244 >> Configuration saved in /mnt/output/checkpoint-50/config.json\n",
      "2023-09-06 14:09:32 - [INFO|modeling_utils.py:1853] 2023-09-06 14:09:32,873 >> Model weights saved in /mnt/output/checkpoint-50/pytorch_model.bin\n",
      "2023-09-06 14:09:32 - [INFO|feature_extraction_utils.py:377] 2023-09-06 14:09:32,875 >> Feature extractor saved in /mnt/output/checkpoint-50/preprocessor_config.json\n",
      "2023-09-06 14:12:31 - {'loss': 0.5214, 'learning_rate': 3.65e-06, 'epoch': 4.76}\n",
      "2023-09-06 14:15:24 - {'loss': 0.2137, 'learning_rate': 4.9000000000000005e-06, 'epoch': 6.35}\n",
      "2023-09-06 14:15:53 - {'eval_loss': 0.5140445232391357, 'eval_wer': 0.28068506184586106, 'eval_runtime': 29.2336, 'eval_samples_per_second': 1.881, 'eval_steps_per_second': 0.137, 'epoch': 6.35}\n",
      " 10% 100/1000 [12:09<1:43:38,  6.91s/it][INFO|trainer.py:776] 2023-09-06 14:15:24,278 >> The following columns in the evaluation set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: input_length. If input_length are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "2023-09-06 14:15:24 - [INFO|trainer.py:3200] 2023-09-06 14:15:24,280 >> ***** Running Evaluation *****\n",
      "2023-09-06 14:15:24 - [INFO|trainer.py:3205] 2023-09-06 14:15:24,280 >>   Batch size = 8\n",
      "2023-09-06 14:15:24 - [INFO|trainer.py:3202] 2023-09-06 14:15:24,280 >>   Num examples = 55\n",
      "2023-09-06 14:15:31 - \n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:07<00:07,  3.64s/it]\u001b[A\n",
      " 75% 3/4 [00:14<00:05,  5.18s/it]\u001b[A\n",
      "                                        \n",
      "100% 4/4 [00:21<00:00,  5.97s/it]\u001b[A\n",
      " 10% 100/1000 [12:39<1:43:38,  6.91s/it]\n",
      "2023-09-06 14:15:53 - [INFO|configuration_utils.py:458] 2023-09-06 14:15:53,565 >> Configuration saved in /mnt/output/checkpoint-100/config.json\n",
      "                                 \u001b[A[INFO|trainer.py:2926] 2023-09-06 14:15:53,564 >> Saving model checkpoint to /mnt/output/checkpoint-100\n",
      "2023-09-06 14:15:53 - [INFO|configuration_utils.py:364] 2023-09-06 14:15:53,566 >> Configuration saved in /mnt/output/checkpoint-100/generation_config.json\n",
      "2023-09-06 14:15:56 - [INFO|modeling_utils.py:1853] 2023-09-06 14:15:56,193 >> Model weights saved in /mnt/output/checkpoint-100/pytorch_model.bin\n",
      "2023-09-06 14:15:56 - [INFO|feature_extraction_utils.py:377] 2023-09-06 14:15:56,194 >> Feature extractor saved in /mnt/output/checkpoint-100/preprocessor_config.json\n",
      "2023-09-06 14:18:54 - {'loss': 0.0798, 'learning_rate': 6.15e-06, 'epoch': 7.94}\n",
      " 15% 150/1000 [18:33<1:38:10,  6.93s/it][INFO|trainer.py:776] 2023-09-06 14:21:47,439 >> The following columns in the evaluation set don't have a corresponding argument in `WhisperForConditionalGeneration.forward` and have been ignored: input_length. If input_length are not expected by `WhisperForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "2023-09-06 14:21:47 - [INFO|trainer.py:3200] 2023-09-06 14:21:47,441 >> ***** Running Evaluation *****\n",
      "2023-09-06 14:21:47 - [INFO|trainer.py:3205] 2023-09-06 14:21:47,441 >>   Batch size = 8\n",
      "2023-09-06 14:21:47 - [INFO|trainer.py:3202] 2023-09-06 14:21:47,441 >>   Num examples = 55\n",
      "2023-09-06 14:21:51 - \n",
      "  0% 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50% 2/4 [00:04<00:04,  2.08s/it]\u001b[A\n",
      " 75% 3/4 [00:09<00:03,  3.27s/it]\u001b[A\n",
      " 15% 150/1000 [18:50<1:38:10,  6.93s/it]\n",
      "                                        \n",
      "2023-09-06 14:22:05 - [INFO|configuration_utils.py:458] 2023-09-06 14:22:05,196 >> Configuration saved in /mnt/output/checkpoint-150/config.json\n",
      "                                 \u001b[A[INFO|trainer.py:2926] 2023-09-06 14:22:05,195 >> Saving model checkpoint to /mnt/output/checkpoint-150\n",
      "100% 4/4 [00:13<00:00,  3.64s/it]\u001b[A\n",
      "2023-09-06 14:22:05 - [INFO|configuration_utils.py:364] 2023-09-06 14:22:05,197 >> Configuration saved in /mnt/output/checkpoint-150/generation_config.json\n",
      "2023-09-06 14:22:07 - [INFO|modeling_utils.py:1853] 2023-09-06 14:22:07,813 >> Model weights saved in /mnt/output/checkpoint-150/pytorch_model.bin\n",
      "2023-09-06 14:22:07 - [INFO|feature_extraction_utils.py:377] 2023-09-06 14:22:07,814 >> Feature extractor saved in /mnt/output/checkpoint-150/preprocessor_config.json\n",
      "2023-09-06 14:21:47 - {'loss': 0.0343, 'learning_rate': 7.4e-06, 'epoch': 9.52}\n",
      "2023-09-06 14:22:05 - {'eval_loss': 0.5870081782341003, 'eval_wer': 0.1665080875356803, 'eval_runtime': 17.7517, 'eval_samples_per_second': 3.098, 'eval_steps_per_second': 0.225, 'epoch': 9.52}\n",
      "2023-09-06 14:25:05 - {'loss': 0.0236, 'learning_rate': 8.65e-06, 'epoch': 11.11}\n"
     ]
    }
   ],
   "source": [
    "job_run.watch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a04f3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stderr cap\n",
    "job_run.watch()\n",
    "\n",
    "with open('output1000_atco2.txt', 'w') as f:\n",
    "    f.write(cap.stdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eaa7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch110_p38_gpu_v1]",
   "language": "python",
   "name": "conda-env-pytorch110_p38_gpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
